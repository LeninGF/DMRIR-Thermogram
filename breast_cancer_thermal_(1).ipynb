{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "breast-cancer-thermal (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-07T19:24:27.993629Z",
          "iopub.execute_input": "2021-07-07T19:24:27.993986Z",
          "iopub.status.idle": "2021-07-07T19:26:04.368315Z",
          "shell.execute_reply.started": "2021-07-07T19:24:27.993910Z",
          "shell.execute_reply": "2021-07-07T19:26:04.367316Z"
        },
        "trusted": true,
        "id": "HKDE5fZ4p6au",
        "outputId": "51485257-d482-4d05-e5b1-8d698d93a7b8"
      },
      "source": [
        "!wget http://visual.ic.uff.br/proeng/thiagoelias/database.rar\n",
        "\n",
        "import os,glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import re\n",
        "import datetime\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
        "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,MaxPooling2D,ReLU,Conv2DTranspose\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.svm import LinearSVC\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog,local_binary_pattern\n",
        "from skimage import data, exposure\n",
        "from skimage.transform import radon, rescale\n",
        "from skimage.filters import roberts, sobel, scharr, prewitt\n",
        "from skimage import feature\n",
        "\n",
        "!pip install rarfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "--2021-07-07 19:24:28--  http://visual.ic.uff.br/proeng/thiagoelias/database.rar\nResolving visual.ic.uff.br (visual.ic.uff.br)... 200.20.15.38\nConnecting to visual.ic.uff.br (visual.ic.uff.br)|200.20.15.38|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 483263536 (461M)\nSaving to: ‘database.rar’\n\ndatabase.rar        100%[===================>] 460.88M  4.43MB/s    in 81s     \n\n2021-07-07 19:25:49 (5.71 MB/s) - ‘database.rar’ saved [483263536/483263536]\n\nCollecting rarfile\n  Downloading rarfile-4.0-py3-none-any.whl (28 kB)\nInstalling collected packages: rarfile\nSuccessfully installed rarfile-4.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "88BHEEilp6aw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.370089Z",
          "iopub.execute_input": "2021-07-07T19:26:04.370441Z",
          "iopub.status.idle": "2021-07-07T19:26:04.765699Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.370400Z",
          "shell.execute_reply": "2021-07-07T19:26:04.764869Z"
        },
        "trusted": true,
        "id": "H0kqEQkrp6ax"
      },
      "source": [
        "string = [138,179,180,181,192,198,202,204,209,210,213,240,241,245,249,250,251,255,257,259,261,263,264,270,400,450,500,550,600]\n",
        "affected_files_text = []\n",
        "for f in string:\n",
        "    list1 = []\n",
        "    normal_dir = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/DOENTES/{}/Segmentadas\".format(f)\n",
        "    normal_dir_text = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/DOENTES/{}/Matrizes\".format(f)\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    normal_files = glob.glob(dir1)\n",
        "    dir2 = os.path.join(normal_dir_text,\"*.txt\")\n",
        "    normal_files_text = glob.glob(dir2)\n",
        "\n",
        "    try:\n",
        "        for i in range(len(normal_files)):\n",
        "            list1.append(normal_files[i]+\"|\"+normal_files_text[i])\n",
        "    except:\n",
        "        print(f)\n",
        "            \n",
        "    affected_files_text.extend(list1)\n",
        "    \n",
        "string2 = [1000,137,166,220,226,42,49,51,55,66,68,750,800,850,900]\n",
        "\n",
        "healthy_files_text = []\n",
        "for f in string2:\n",
        "    list2 = []\n",
        "    normal_dir = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/SAUDA╠üVEIS/{}/Segmentadas\".format(f)\n",
        "    normal_dir_text = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/SAUDA╠üVEIS/{}/Matrizes\".format(f)\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    normal_files = glob.glob(dir1)\n",
        "    dir2 = os.path.join(normal_dir_text,\"*.txt\")\n",
        "    normal_files_text = glob.glob(dir2)\n",
        "    #print(len(normal_files),f)\n",
        "    try:\n",
        "        for i in range(len(normal_files)):\n",
        "            list2.append(normal_files[i]+\"|\"+normal_files_text[i])\n",
        "    except:\n",
        "        print(f)\n",
        "            \n",
        "    healthy_files_text.extend(list2)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoXL4di9p6ax"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.768893Z",
          "iopub.execute_input": "2021-07-07T19:26:04.769157Z",
          "iopub.status.idle": "2021-07-07T19:26:04.811277Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.769130Z",
          "shell.execute_reply": "2021-07-07T19:26:04.810561Z"
        },
        "trusted": true,
        "id": "mf4BsnF6p6az"
      },
      "source": [
        "string = [650,700]\n",
        "affected_files_text2 = []\n",
        "for f in string:\n",
        "    list1 = []\n",
        "    normal_dir = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/DOENTES/{}/Segmentadas\".format(f)\n",
        "    normal_dir_text = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/DOENTES/{}/Matrizes\".format(f)\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    normal_files = glob.glob(dir1)\n",
        "    dir2 = os.path.join(normal_dir_text,\"*.txt\")\n",
        "    normal_files_text = glob.glob(dir2)\n",
        "\n",
        "    try:\n",
        "        for i in range(len(normal_files)):\n",
        "            list1.append(normal_files[i]+\"|\"+normal_files_text[i])\n",
        "    except:\n",
        "        print(f)\n",
        "            \n",
        "    affected_files_text2.extend(list1)\n",
        "    \n",
        "string2 = [950]\n",
        "\n",
        "healthy_files_text2 = []\n",
        "for f in string2:\n",
        "    list2 = []\n",
        "    normal_dir = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/SAUDA╠üVEIS/{}/Segmentadas\".format(f)\n",
        "    normal_dir_text = \"../input/thermal-images-for-breast-cancer-diagnosis-dmrir/Imagens e Matrizes da Tese de Thiago Alves Elias da Silva/Desenvolvimento da Metodologia/SAUDA╠üVEIS/{}/Matrizes\".format(f)\n",
        "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
        "    normal_files = glob.glob(dir1)\n",
        "    dir2 = os.path.join(normal_dir_text,\"*.txt\")\n",
        "    normal_files_text = glob.glob(dir2)\n",
        "    #print(len(normal_files),f)\n",
        "    try:\n",
        "        for i in range(len(normal_files)):\n",
        "            list2.append(normal_files[i]+\"|\"+normal_files_text[i])\n",
        "    except:\n",
        "        print(f)\n",
        "            \n",
        "    healthy_files_text2.extend(list2)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.904851Z",
          "iopub.execute_input": "2021-07-07T19:26:04.905100Z",
          "iopub.status.idle": "2021-07-07T19:26:04.910737Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.905073Z",
          "shell.execute_reply": "2021-07-07T19:26:04.909887Z"
        },
        "trusted": true,
        "id": "uFDNQ4sLp6a1"
      },
      "source": [
        "affected_files_test.extend(affected_files_text2)\n",
        "healthy_files_test.extend(healthy_files_text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.913554Z",
          "iopub.execute_input": "2021-07-07T19:26:04.913835Z",
          "iopub.status.idle": "2021-07-07T19:26:04.923512Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.913800Z",
          "shell.execute_reply": "2021-07-07T19:26:04.922737Z"
        },
        "trusted": true,
        "id": "A7Fn-grfp6a1"
      },
      "source": [
        "print(len(healthy_files_text))\n",
        "print(len(affected_files_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.925706Z",
          "iopub.execute_input": "2021-07-07T19:26:04.926071Z",
          "iopub.status.idle": "2021-07-07T19:26:04.932951Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.926036Z",
          "shell.execute_reply": "2021-07-07T19:26:04.931851Z"
        },
        "trusted": true,
        "id": "_XNDzlqFp6a2"
      },
      "source": [
        "print(len(healthy_files_test))\n",
        "print(len(affected_files_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.935914Z",
          "iopub.execute_input": "2021-07-07T19:26:04.936396Z",
          "iopub.status.idle": "2021-07-07T19:26:04.943945Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.936262Z",
          "shell.execute_reply": "2021-07-07T19:26:04.943196Z"
        },
        "trusted": true,
        "id": "0KVMTiXKp6a2"
      },
      "source": [
        "train_dic = {}\n",
        "for f in affected_files_text:\n",
        "  train_dic[f] = [1,0]\n",
        "for f in healthy_files_text:\n",
        "  train_dic[f] = [0,1]\n",
        "\n",
        "\n",
        "test_dic = {}\n",
        "for f in affected_files_test:\n",
        "  test_dic[f] = [1,0]\n",
        "for f in healthy_files_test:\n",
        "  test_dic[f] = [0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.945586Z",
          "iopub.execute_input": "2021-07-07T19:26:04.945934Z",
          "iopub.status.idle": "2021-07-07T19:26:04.952818Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.945899Z",
          "shell.execute_reply": "2021-07-07T19:26:04.951969Z"
        },
        "trusted": true,
        "id": "GOteQuE8p6a2"
      },
      "source": [
        "import random\n",
        "l_train = list(train_dic.items())\n",
        "random.shuffle(l_train)\n",
        "\n",
        "\n",
        "import random\n",
        "l_test = list(test_dic.items())\n",
        "random.shuffle(l_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tnBQ5n0p6a2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BkQku24p6a2"
      },
      "source": [
        "# #ROBERTS AND SOBEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPcjr-1bp6a2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:26:04.954194Z",
          "iopub.execute_input": "2021-07-07T19:26:04.954635Z",
          "iopub.status.idle": "2021-07-07T19:29:04.500401Z",
          "shell.execute_reply.started": "2021-07-07T19:26:04.954597Z",
          "shell.execute_reply": "2021-07-07T19:29:04.499471Z"
        },
        "trusted": true,
        "id": "g7HHXLoIp6a3"
      },
      "source": [
        "data = []\n",
        "normal_data = []\n",
        "labels = []\n",
        "for i in range(len(l_train)):\n",
        "    file_name,label = l_train[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr[i] = arr[i].replace(\",\", \".\")\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1)\n",
        "        edge_roberts = roberts(arr_1)\n",
        "        edge_sobel = sobel(arr_1)\n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = cv2.resize(edge_roberts,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = cv2.resize(edge_sobel,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        data.append(img2)\n",
        "        normal_data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "train_data = np.array(data)\n",
        "print(train_data.shape)\n",
        "normal_data = np.array(normal_data)\n",
        "print(normal_data.shape)\n",
        "\n",
        "train_labels = np.array(labels)\n",
        "print(train_labels.shape)    \n",
        "\n",
        "print('^_^-training data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:29:04.501709Z",
          "iopub.execute_input": "2021-07-07T19:29:04.502202Z",
          "iopub.status.idle": "2021-07-07T19:29:54.962402Z",
          "shell.execute_reply.started": "2021-07-07T19:29:04.502164Z",
          "shell.execute_reply": "2021-07-07T19:29:54.960889Z"
        },
        "trusted": true,
        "id": "uc8cEZiRp6a3"
      },
      "source": [
        "test_text = []\n",
        "data = []\n",
        "labels = []\n",
        "for i in range(len(l_test)):\n",
        "    file_name,label = l_test[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1) \n",
        "        edge_roberts = roberts(arr_1)\n",
        "        edge_sobel = sobel(arr_1) \n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_AREA)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = cv2.resize(edge_roberts,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = cv2.resize(edge_sobel,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        test_text.append(img2)\n",
        "        data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "  \n",
        "       \n",
        "test_data = np.array(data)\n",
        "print(test_data.shape)\n",
        "test_text = np.array(test_text)\n",
        "print(test_text.shape)\n",
        "test_labels = np.array(labels)\n",
        "print(test_labels.shape)    \n",
        "\n",
        "print('^_^-testing data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpe_FnHUp6a4"
      },
      "source": [
        "# **#CANNY AND roberts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T16:59:54.975091Z",
          "iopub.execute_input": "2021-07-07T16:59:54.975422Z",
          "iopub.status.idle": "2021-07-07T17:04:46.778562Z",
          "shell.execute_reply.started": "2021-07-07T16:59:54.975396Z",
          "shell.execute_reply": "2021-07-07T17:04:46.777612Z"
        },
        "trusted": true,
        "id": "S0eF_Omwp6a4"
      },
      "source": [
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "data = []\n",
        "normal_data = []\n",
        "labels = []\n",
        "for i in range(len(l_train)):\n",
        "    file_name,label = l_train[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr[i] = arr[i].replace(\",\", \".\")\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1)\n",
        "        edge_roberts = roberts(arr_1)\n",
        "        edge_canny = feature.canny(arr_1, sigma=1)\n",
        "        edge_canny = np.asarray(edge_canny)\n",
        "        \n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = cv2.resize(edge_roberts,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = resize(edge_canny, (224,224),anti_aliasing=True)\n",
        "        #s_img = cv2.resize(edge_canny,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        \n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        data.append(img2)\n",
        "        normal_data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "train_data = np.array(data)\n",
        "print(train_data.shape)\n",
        "normal_data = np.array(normal_data)\n",
        "print(normal_data.shape)\n",
        "\n",
        "train_labels = np.array(labels)\n",
        "print(train_labels.shape)    \n",
        "\n",
        "print('^_^-training data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T17:04:46.779965Z",
          "iopub.execute_input": "2021-07-07T17:04:46.780466Z",
          "iopub.status.idle": "2021-07-07T17:06:04.66699Z",
          "shell.execute_reply.started": "2021-07-07T17:04:46.780423Z",
          "shell.execute_reply": "2021-07-07T17:06:04.666041Z"
        },
        "trusted": true,
        "id": "skSDNAUdp6a4"
      },
      "source": [
        "test_text = []\n",
        "data = []\n",
        "labels = []\n",
        "for i in range(len(l_test)):\n",
        "    file_name,label = l_test[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1) \n",
        "        edge_roberts = roberts(arr_1)\n",
        "        edge_canny = feature.canny(arr_1, sigma=1)\n",
        "        edge_canny = np.asarray(edge_canny)\n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_AREA)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = cv2.resize(edge_roberts,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = resize(edge_canny, (224,224),anti_aliasing=True)\n",
        "        #r_img = cv2.resize(edge_canny,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        test_text.append(img2)\n",
        "        data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "  \n",
        "       \n",
        "test_data = np.array(data)\n",
        "print(test_data.shape)\n",
        "test_text = np.array(test_text)\n",
        "print(test_text.shape)\n",
        "test_labels = np.array(labels)\n",
        "print(test_labels.shape)    \n",
        "\n",
        "print('^_^-testing data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymSLXYX0p6a5"
      },
      "source": [
        "# **#CANNY AND prewitt**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Nf0Br42Tp6a5"
      },
      "source": [
        "data = []\n",
        "normal_data = []\n",
        "labels = []\n",
        "for i in range(len(l_train)):\n",
        "    file_name,label = l_train[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr[i] = arr[i].replace(\",\", \".\")\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1)\n",
        "        edge_prewitt = prewitt(arr_1)\n",
        "        edge_canny = feature.canny(arr_1, sigma=1)\n",
        "        edge_canny = np.asarray(edge_canny)\n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = cv2.resize(edge_prewitt,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        #s_img = cv2.resize(edge_canny,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = resize(edge_canny, (224,224),anti_aliasing=True)\n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        data.append(img2)\n",
        "        normal_data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "train_data = np.array(data)\n",
        "print(train_data.shape)\n",
        "normal_data = np.array(normal_data)\n",
        "print(normal_data.shape)\n",
        "\n",
        "train_labels = np.array(labels)\n",
        "print(train_labels.shape)    \n",
        "\n",
        "print('^_^-training data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FjaQcsqpp6a5"
      },
      "source": [
        "test_text = []\n",
        "data = []\n",
        "labels = []\n",
        "for i in range(len(l_test)):\n",
        "    file_name,label = l_test[i]\n",
        "    a,b = file_name.split(\"|\")\n",
        "    img = cv2.imread(a)\n",
        "    arr_1 = np.empty((480,640))\n",
        "    try:\n",
        "        with open(b,\"r\") as f:\n",
        "            arr = f.readlines()\n",
        "            for i in range(len(arr)):\n",
        "                arr_2 = []\n",
        "                for j in arr[i].split():\n",
        "                    arr_2.append(float(j))\n",
        "                arr_1[i] = np.asarray(arr_2) \n",
        "        img = np.asarray(img)\n",
        "        arr_1 = np.asarray(arr_1) \n",
        "        edge_prewitt = prewitt(arr_1)\n",
        "        edge_canny = feature.canny(arr_1, sigma=1)\n",
        "        edge_canny = np.asarray(edge_canny)\n",
        "        img = cv2.resize(img,(224,224),interpolation = cv2.INTER_AREA)\n",
        "        arr_img = cv2.resize(arr_1,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        s_img = cv2.resize(edge_prewitt,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        #r_img = cv2.resize(edge_canny,(224,224),interpolation = cv2.INTER_CUBIC)\n",
        "        r_img = resize(edge_canny, (224,224),anti_aliasing=True)\n",
        "        arr_3 = np.expand_dims(arr_img,axis = -1)\n",
        "        arr_2 = np.expand_dims(r_img,axis = -1)\n",
        "        arr_1 = np.expand_dims(s_img,axis = -1)\n",
        "        img2 = np.concatenate((arr_3,arr_2,arr_1),axis = -1)\n",
        "        height, width = img.shape[:2]\n",
        "        img2 = img2.astype('float32')/255.0\n",
        "        img = img.astype('float32')/255.0\n",
        "        test_text.append(img2)\n",
        "        data.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    except:\n",
        "        print(i,file_name)\n",
        "        print(\"Not possible\")  \n",
        "  \n",
        "       \n",
        "test_data = np.array(data)\n",
        "print(test_data.shape)\n",
        "test_text = np.array(test_text)\n",
        "print(test_text.shape)\n",
        "test_labels = np.array(labels)\n",
        "print(test_labels.shape)    \n",
        "\n",
        "print('^_^-testing data finished-^_^')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aYwsLeop6a5"
      },
      "source": [
        "# **Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:29:54.963680Z",
          "iopub.execute_input": "2021-07-07T19:29:54.964008Z",
          "iopub.status.idle": "2021-07-07T19:30:00.464494Z",
          "shell.execute_reply.started": "2021-07-07T19:29:54.963972Z",
          "shell.execute_reply": "2021-07-07T19:30:00.463611Z"
        },
        "trusted": true,
        "id": "gytZb9Ajp6a7",
        "outputId": "89d7ae00-c7e5-4e03-878c-3bfc61985345"
      },
      "source": [
        "in_model = tf.keras.applications.DenseNet121(input_shape=(224,224,3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet',classes = 2)\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "\n",
        "\n",
        "x = in_model(inputs)\n",
        "flat = Flatten()(x)\n",
        "\n",
        "dense_1 = Dense(4096,activation = 'relu')(flat)\n",
        "\n",
        "dense_2 = Dense(4096,activation = 'relu')(dense_1)\n",
        "\n",
        "prediction = Dense(2,activation = 'softmax')(dense_2)\n",
        "\n",
        "in_pred = Model(inputs = inputs,outputs = prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n29089792/29084464 [==============================] - 0s 0us/step\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:30:00.465718Z",
          "iopub.execute_input": "2021-07-07T19:30:00.466054Z",
          "iopub.status.idle": "2021-07-07T19:35:22.190944Z",
          "shell.execute_reply.started": "2021-07-07T19:30:00.466020Z",
          "shell.execute_reply": "2021-07-07T19:35:22.189949Z"
        },
        "trusted": true,
        "id": "g4NrhsoQp6a8"
      },
      "source": [
        "in_pred.summary()\n",
        "in_pred.compile(optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.0002), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False) , metrics=['accuracy'])\n",
        "in_pred.fit(train_data,train_labels,batch_size = 32,epochs = 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T19:35:51.644818Z",
          "iopub.execute_input": "2021-07-07T19:35:51.645216Z",
          "iopub.status.idle": "2021-07-07T19:35:52.264652Z",
          "shell.execute_reply.started": "2021-07-07T19:35:51.645183Z",
          "shell.execute_reply": "2021-07-07T19:35:52.263877Z"
        },
        "trusted": true,
        "id": "H_Y_0g7Ap6a8",
        "outputId": "fe6a3a91-b061-4215-e908-c584b6a1d63b"
      },
      "source": [
        "in_pred.evaluate(test_data,test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "8/8 [==============================] - 0s 56ms/step - loss: 8.5441 - accuracy: 0.4647\n",
          "output_type": "stream"
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[8.544113159179688, 0.46473029255867004]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T18:42:35.253606Z",
          "iopub.execute_input": "2021-07-07T18:42:35.253961Z",
          "iopub.status.idle": "2021-07-07T18:42:36.817627Z",
          "shell.execute_reply.started": "2021-07-07T18:42:35.253928Z",
          "shell.execute_reply": "2021-07-07T18:42:36.816703Z"
        },
        "trusted": true,
        "id": "MVhdXEmlp6a8"
      },
      "source": [
        "test_ = in_pred.predict(test_text)\n",
        "Y_pred= np.argmax(test_labels, axis=1)\n",
        "vgg19 = np.argmax(test_, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-07T18:42:36.819138Z",
          "iopub.execute_input": "2021-07-07T18:42:36.819471Z",
          "iopub.status.idle": "2021-07-07T18:42:36.840176Z",
          "shell.execute_reply.started": "2021-07-07T18:42:36.819435Z",
          "shell.execute_reply": "2021-07-07T18:42:36.839392Z"
        },
        "trusted": true,
        "id": "FIzU9g0up6a8"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "print(confusion_matrix(Y_pred,vgg19))\n",
        "print(classification_report(Y_pred,vgg19))\n",
        "print(accuracy_score(Y_pred,vgg19))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}